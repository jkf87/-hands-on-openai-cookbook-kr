{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 요금 한도를 처리하는 방법\n",
    "\n",
    "OpenAI API를 반복적으로 호출하면 `429: '요청이 너무 많습니다'` 또는 `RateLimitError`라는 오류 메시지가 표시될 수 있습니다. 이러한 오류 메시지는 API의 속도 제한을 초과하여 발생합니다.\n",
    "\n",
    "이 가이드에서는 전송률 제한 오류를 방지하고 처리하기 위한 팁을 공유합니다.\n",
    "\n",
    "속도 제한 오류를 피하기 위해 병렬 요청을 스로틀링하는 스크립트 예제를 보려면 [api_request_parallel_processor.py](api_request_parallel_processor.py)를 참조하세요.\n",
    "\n",
    "## 속도 제한이 존재하는 이유\n",
    "\n",
    "속도 제한은 API의 일반적인 관행이며, 몇 가지 다른 이유로 인해 설정됩니다.\n",
    "\n",
    "- 첫째, API의 남용 또는 오용을 방지하는 데 도움이 됩니다. 예를 들어, 악의적인 공격자가 API에 과부하를 일으키거나 서비스 중단을 유발하기 위해 요청을 폭주시킬 수 있습니다. 속도 제한을 설정하면 이러한 종류의 활동을 방지할 수 있습니다.\n",
    "- 둘째, 속도 제한은 모든 사람이 API에 공정하게 액세스할 수 있도록 보장합니다. 한 사람이나 조직이 과도하게 많은 요청을 하면 다른 모든 사람의 API가 느려질 수 있습니다. OpenAI는 한 사용자가 요청할 수 있는 요청 수를 제한함으로써 모든 사용자가 속도 저하 없이 API를 사용할 수 있도록 보장합니다.\n",
    "- 마지막으로, 속도 제한은 OpenAI가 인프라의 총 부하를 관리하는 데 도움이 될 수 있습니다. API에 대한 요청이 급격히 증가하면 서버에 부담을 주고 성능 문제를 일으킬 수 있습니다. 속도 제한을 설정하면 OpenAI는 모든 사용자에게 원활하고 일관된 환경을 유지할 수 있습니다.\n",
    "\n",
    "속도 제한에 도달하는 것은 실망스러울 수 있지만, 속도 제한은 사용자를 위한 API의 안정적인 운영을 보호하기 위해 존재합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 요율 한도 ##\n",
    "\n",
    "2023년 1월부터 기본 이자율 한도는 다음과 같습니다:\n",
    "\n",
    "<표>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>텍스트 완성 및 엔드포인트 임베딩</th>\n",
    "    <th>코드 및 엔드포인트 편집</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>무료 평가판 사용자</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 요청/분</li>\n",
    "            <li>150,000토큰/분</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 요청/분</li>\n",
    "            <li>150,000토큰/분</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>종량제 사용자(첫 48시간 내)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>60 요청/분</li>\n",
    "            <li>250,000 다빈치 토큰/분(더 저렴한 모델의 경우 비례적으로 더 많음)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 요청/분</li>\n",
    "            <li>150,000토큰/분</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>종량제 사용자(첫 48시간 이후)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>3,000 요청/분</li>\n",
    "            <li>250,000 다빈치 토큰/분(더 저렴한 모델의 경우 비례적으로 더 많음)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 요청/분</li>\n",
    "            <li>150,000토큰/분</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "참고로 1,000개의 토큰은 대략 텍스트 한 페이지에 해당합니다.\n",
    "\n",
    "### 기타 속도 제한 리소스\n",
    "\n",
    "다른 리소스에서 OpenAI의 속도 제한에 대해 자세히 알아보세요:\n",
    "\n",
    "- 가이드: 속도 제한](https://beta.openai.com/docs/guides/rate-limits/overview)\n",
    "- 도움말 센터: API 사용에 속도 제한이 적용되나요?(https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits)\n",
    "- 도움말 센터: 429: '너무 많은 요청' 오류를 해결하려면 어떻게 해야 하나요?](https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors)\n",
    "\n",
    "### 비율 제한 증가 요청하기\n",
    "\n",
    "조직의 요금 한도를 늘리려면 다음 양식을 작성해 주세요:\n",
    "\n",
    "- OpenAI 요금 한도 증액 요청 양식](https://forms.gle/56ZrwXXoxAN1yt6i9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 속도 제한 오류 예시\n",
    "\n",
    "API 요청이 너무 빨리 전송되면 속도 제한 오류가 발생합니다. OpenAI Python 라이브러리를 사용하는 경우 다음과 같이 표시됩니다:\n",
    "\n",
    "```\n",
    "RateLimitError: 조직 org-{id}의 기본 코덱스에 대한 분당 요청 수에 대한 속도 제한에 도달했습니다. 제한: 20.000000/분. 현재: 24.000000/분. 문제가 계속되거나 증가를 요청하려면 support@openai.com 으로 문의하세요.\n",
    "```\n",
    "\n",
    "다음은 속도 제한 오류를 트리거하는 예제 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "# request a bunch of completions in a loop\n",
    "for _ in range(100):\n",
    "    openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "속도 제한 오류를 방지하는 방법 ##\n",
    "\n",
    "지수 백오프로 재시도하기 ###\n",
    "\n",
    "비율 제한 오류를 피하는 쉬운 방법 중 하나는 무작위 지수 백오프를 사용하여 요청을 자동으로 재시도하는 것입니다. 지수 백오프를 사용하여 재시도한다는 것은 비율 제한 오류가 발생하면 짧은 절전을 수행한 다음 실패한 요청을 다시 시도하는 것을 의미합니다. 요청이 여전히 실패하면 절전 시간이 길어지고 프로세스가 반복됩니다. 이 과정은 요청이 성공하거나 최대 재시도 횟수에 도달할 때까지 계속됩니다.\n",
    "\n",
    "이 접근 방식에는 많은 이점이 있습니다:\n",
    "\n",
    "- 자동 재시도는 충돌이나 데이터 누락 없이 속도 제한 오류에서 복구할 수 있음을 의미합니다.\n",
    "- 지수 백오프를 사용하면 첫 번째 재시도를 빠르게 시도할 수 있으며, 처음 몇 번의 재시도가 실패할 경우 지연 시간이 길어지는 이점을 누릴 수 있습니다.\n",
    "- 지연에 무작위 지터를 추가하면 모든 히트에서 동시에 재시도하는 데 도움이 됩니다.\n",
    "\n",
    "실패한 요청은 분당 한도에 영향을 미치므로 요청을 계속 재전송하면 작동하지 않습니다.\n",
    "\n",
    "다음은 몇 가지 해결 방법의 예입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예시 #1: 끈기 라이브러리 사용\n",
    "\n",
    "[Tenacity](https://tenacity.readthedocs.io/en/latest/)는 거의 모든 항목에 재시도 동작을 추가하는 작업을 간소화하기 위해 Python으로 작성된 Apache 2.0 라이선스 범용 재시도 라이브러리입니다.\n",
    "\n",
    "요청에 기하급수적 백오프를 추가하려면 `tenacity.retry` [데코레이터](https://peps.python.org/pep-0318/)를 사용하면 됩니다. 다음 예제는 `tenacity.wait_random_exponential` 함수를 사용하여 요청에 무작위 지수 백오프를 추가하는 예제입니다.\n",
    "\n",
    "Tenacity 라이브러리는 타사 도구이며, OpenAI는 안정성이나 보안에 대해 보장하지 않는다는 점에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5 at 0x10d8cae00> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" a little girl dreamed of becoming a model.\\n\\nNowadays, that dream\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793900,\n",
       "  \"id\": \"cmpl-5oowO391reUW8RGVfFyzBM1uBs4A5\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 #2: 백오프 라이브러리 사용\n",
    "\n",
    "백오프 및 재시도를 위한 함수 데코레이터를 제공하는 또 다른 라이브러리는 [backoff](https://pypi.org/project/backoff/)입니다.\n",
    "\n",
    "Tenacity와 마찬가지로 백오프 라이브러리는 타사 도구이며, OpenAI는 안정성이나 보안에 대해 보장하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M at 0x111043680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" two children lived in a poor country village. In the winter, the temperature would\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793901,\n",
       "  \"id\": \"cmpl-5oowPhIdUvshEsF1rBhhwE9KFfI3M\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import backoff  # for exponential backoff\n",
    "import openai  # for OpenAI API calls\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 3: 수동 백오프 구현\n",
    "\n",
    "타사 라이브러리를 사용하고 싶지 않다면 자체 백오프 로직을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT at 0x111024220> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" a man decided to greatly improve his karma by turning his life around.\\n\\n\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793903,\n",
       "  \"id\": \"cmpl-5oowRsCXv3AkUgVJyyo3TQrVq7hIT\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "import openai\n",
    "\n",
    "# define a retry decorator\n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    errors: tuple = (openai.error.RateLimitError,),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specified errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@retry_with_exponential_backoff\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "속도 제한이 주어진 일괄 처리의 처리량을 최대화하는 방법 ##\n",
    "\n",
    "사용자의 실시간 요청을 처리하는 경우, 백오프 후 재시도는 속도 제한 오류를 방지하면서 지연 시간을 최소화할 수 있는 훌륭한 전략입니다.\n",
    "\n",
    "그러나 지연 시간보다 처리량이 더 중요한 대량의 배치 데이터를 처리하는 경우, 백오프 및 재시도 외에 몇 가지 다른 방법을 사용할 수 있습니다.\n",
    "\n",
    "### 요청 사이에 사전적으로 지연 시간 추가하기\n",
    "\n",
    "속도 제한에 도달했다가 백오프하고, 다시 속도 제한에 도달하고, 다시 백오프하는 일이 반복되면 요청 예산의 상당 부분이 재시도해야 하는 요청에 '낭비'될 수 있습니다. 이렇게 하면 고정된 비율 제한이 주어졌을 때 처리량이 제한됩니다.\n",
    "\n",
    "여기서 한 가지 잠재적인 해결책은 속도 제한을 계산하고 그 역수와 동일한 지연을 추가하는 것입니다(예: 속도 제한이 분당 요청 20건인 경우 각 요청에 3~6초의 지연을 추가). 이렇게 하면 속도 제한 한도에 도달하여 낭비되는 요청을 발생시키지 않고 한도 근처에서 운영할 수 있습니다.\n",
    "\n",
    "#### 요청에 지연을 추가하는 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp at 0x11b2c7680> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" there was an idyllic little farm that sat by a babbling brook\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1662793907,\n",
       "  \"id\": \"cmpl-5oowVVZnAzdCPtUJ0rifeamtLcZRp\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Define a function that adds a delay to a Completion API call\n",
    "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
    "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
    "\n",
    "    # Sleep for the delay\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 20\n",
    "delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    "delayed_completion(\n",
    "    delay_in_seconds=delay,\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Once upon a time,\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 일괄 처리 요청\n",
    "\n",
    "OpenAI API에는 분당 요청 수와 분당 토큰 수에 대한 별도의 제한이 있습니다.\n",
    "\n",
    "분당 요청 수 제한에 도달했지만 분당 토큰 수에 여유가 있는 경우, 여러 작업을 각 요청에 일괄 처리하여 처리량을 늘릴 수 있습니다. 이렇게 하면 특히 소규모 모델에서 분당 더 많은 토큰을 처리할 수 있습니다.\n",
    "\n",
    "프롬프트 일괄 전송은 단일 문자열 대신 '프롬프트' 매개변수에 문자열 목록을 전달한다는 점을 제외하면 일반 API 호출과 완전히 동일하게 작동합니다.\n",
    "\n",
    "**경고: 응답 객체가 프롬프트 순서대로 완료를 반환하지 않을 수 있으므로 항상 `index` 필드를 사용하여 응답을 프롬프트에 다시 일치시켜야 합니다.\n",
    "\n",
    "#### 일괄 처리하지 않은 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, before there were grandiloquent tales of the massacre at Fort Mims, there were stories of\n",
      "Once upon a time, a full-sized search and rescue was created. However, CIDIs are the addition of requiring\n",
      "Once upon a time, Schubert was hot with the films. “Schubert sings of honey, flowers,\n",
      "Once upon a time, you could watch these films on your VCR, sometimes years after their initial theatrical release, and there\n",
      "Once upon a time, there was a forest. In that forest, the forest animals ruled. The forest animals had their homes\n",
      "Once upon a time, there were two programs that complained about false positive scans. Peacock and Midnight Manager alike, only\n",
      "Once upon a time, a long, long time ago, tragedy struck. it was the darkest of nights, and there was\n",
      "Once upon a time, when Adam was a perfect little gentleman, he was presented at Court as a guarantee of good character.\n",
      "Once upon a time, Adam and Eve made a mistake. They ate the fruit from the tree of immortality and split the consequences\n",
      "Once upon a time, there was a set of programming fundamental principles known as the \"X model.\" This is a set of\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "prompt = \"Once upon a time,\"\n",
    "\n",
    "# serial example, with one story completion per request\n",
    "for _ in range(num_stories):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "\n",
    "    # print story\n",
    "    print(prompt + response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일괄 처리 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there were two sisters, Eliza Pickering and Ariana 'Ari' Lucas. When these lovely\n",
      "Once upon a time, Keene was stung by a worm — actually, probably a python — snaking through his leg\n",
      "Once upon a time, there was a professor of physics during the depression. It was difficult, during this time, to get\n",
      "Once upon a time, before you got sick, you told stories to all and sundry, and your listeners believed in you\n",
      "Once upon a time, there was one very old nice donkey. He was incredibly smart, in a very old, kind of\n",
      "Once upon a time, the property of a common lodging house was a common cup for all the inhabitants. Betimes a constant\n",
      "Once upon a time, in an unspecified country, there was a witch who had an illegal product. It was highly effective,\n",
      "Once upon a time, a long time ago, I turned 13, my beautiful dog Duncan swept me up into his jaws like\n",
      "Once upon a time, as a thoroughly reformed creature from an army of Nazis, he took On Judgement Day myself and his\n",
      "Once upon a time, Capcom made a game for the Atari VCS called Missile Command. While it was innovative at the time\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "prompts = [\"Once upon a time,\"] * num_stories\n",
    "\n",
    "# batched example, with 10 stories completions per request\n",
    "response = openai.Completion.create(\n",
    "    model=\"curie\",\n",
    "    prompt=prompts,\n",
    "    max_tokens=20,\n",
    ")\n",
    "\n",
    "# match completions to prompts by index\n",
    "stories = [\"\"] * len(prompts)\n",
    "for choice in response.choices:\n",
    "    stories[choice.index] = prompts[choice.index] + choice.text\n",
    "\n",
    "# print stories\n",
    "for story in stories:\n",
    "    print(story)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬 처리 스크립트 예제\n",
    "\n",
    "대량의 API 요청을 병렬 처리하기 위한 예제 스크립트를 작성했습니다: [api_request_parallel_processor.py](https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py).\n",
    "\n",
    "이 스크립트에는 몇 가지 편리한 기능이 결합되어 있습니다:\n",
    "- 대용량 작업으로 인한 메모리 부족을 방지하기 위해 파일에서 요청을 스트리밍합니다.\n",
    "- 처리량을 최대화하기 위해 요청을 동시에 수행합니다.\n",
    "- 요청과 토큰 사용량을 모두 스로틀하여 속도 제한을 준수합니다.\n",
    "- 데이터 누락을 방지하기 위해 실패한 요청을 재시도합니다.\n",
    "- 오류를 기록하여 요청 문제를 진단합니다.\n",
    "\n",
    "있는 그대로 사용하거나 필요에 맞게 자유롭게 수정하세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('openai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9 (main, Dec  7 2021, 18:04:56) \n[Clang 13.0.0 (clang-1300.0.29.3)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
